# -*- coding: utf-8 -*-
"""GeminiFinancialDecisionSupporter

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/geminifinancialdecisionsupporter-cb1f630f-2829-42fe-84ec-6e6256dca33d.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20241129/auto/storage/goog4_request%26X-Goog-Date%3D20241129T042635Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D24446e65da771b2e9a3f11712e6bafa45f6b2d672bd64cc4853b0514477eb9fd45eaaece17d31a44b0cd109a2c330c7bdaac6f33d21aca51fd68a719d74613a350f57b08b26c13e7b652f3549909abb23bd0443c771d32847babbf5dae079fa45999e77ea30733617d8cd382b83e1f34ad7da42251ed83ef5576461f3ee046dea822392256e3089df1d1f460b15ae53ebfe13bbc3b67e65eca1d78f97946857223c9898ee38a6fe0d31c39cc3b84cb2ac6750e60a69c69c3004c2cdedffad3db6fcaf0f45705bdadcd500044a6d76b4214ea90c95b0725978e0a40e9124c1db2445432ee0321091018bb5aaac2319a3a996f8fd1ec4dd2074810f403433e0c74
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

gemini_long_context_path = kagglehub.competition_download('gemini-long-context')

print('Data source import complete.')

!pip install dash
!pip install feedparser
!pip install kaleido

import feedparser
import yaml
import datetime
from datetime import timedelta
import pytz
import os

import yaml
import pandas as pd
from textblob import TextBlob

import os
import yaml
import time
from google.generativeai import GenerativeModel
from google.generativeai import caching
from google.generativeai import configure

import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import yaml
from dash import Dash, html, dcc, Input, Output

import google.generativeai as genai
from google.generativeai import caching
from kaggle_secrets import UserSecretsClient
import datetime

# Imports
import os
import yaml
import feedparser
import pandas as pd
import pytz
from datetime import datetime
from textblob import TextBlob
from dash import Dash, html, dcc, Input, Output
import plotly.express as px
import google.generativeai as genai
from kaggle_secrets import UserSecretsClient
import time

import functools
import time
import feedparser
import plotly.io as pio

# GEMINI API Configuration
def configure_gemini_api():
    # Retrieve GEMINI API Key using Kaggle Secrets
    user_secrets = UserSecretsClient()
    gemini_api_key = user_secrets.get_secret("GEMINI")

    # Validate secret retrieval
    if not gemini_api_key:
        raise ValueError("GEMINI API Key not found in Kaggle Secrets. Please ensure it's set correctly.")

    # Configure GEMINI API
    genai.configure(api_key=gemini_api_key)
    print("GEMINI API configured successfully!")



def fetch_bbc_headlines(category='world'):
    rss_url = f'http://feeds.bbci.co.uk/news/{category}/rss.xml'
    feed = feedparser.parse(rss_url)
    return feed.entries

def filter_headlines_by_time(entries, start_time, end_time):
    filtered_entries = []
    for entry in entries:
        published_time = datetime.datetime(*entry.published_parsed[:6], tzinfo=pytz.UTC)
        published_time = published_time.astimezone(pytz.timezone('Europe/London'))
        if start_time <= published_time <= end_time:
            filtered_entries.append({
                'title': entry.title,
                'link': entry.link,
                'published': published_time.isoformat(),
                'category': entry.get('category', 'General')
            })
    return filtered_entries

def fetch_and_store_headlines(categories):
    now_uk = datetime.datetime.now(pytz.timezone('Europe/London'))
    start_time = now_uk.replace(hour=0, minute=0, second=0, microsecond=0)
    end_time = now_uk
    all_headlines = []
    for category in categories:
        entries = fetch_bbc_headlines(category)
        filtered_entries = filter_headlines_by_time(entries, start_time, end_time)
        all_headlines.extend(filtered_entries)
    save_headlines_to_yaml(all_headlines)

def save_headlines_to_yaml(headlines, filename='bbc_headlines.yaml'):
    if os.path.exists(filename):
        with open(filename, 'r') as file:
            existing_data = yaml.safe_load(file) or []
    else:
        existing_data = []
    combined_data = existing_data + headlines
    seen_titles = set()
    unique_data = []
    for item in combined_data:
        if item['title'] not in seen_titles:
            unique_data.append(item)
            seen_titles.add(item['title'])
    with open(filename, 'w') as file:
        yaml.dump(unique_data, file, sort_keys=False)

# Caching Implementation
class SimpleCache:
    def __init__(self, expiration_seconds=3600):
        self.cache = {}
        self.expiration_seconds = expiration_seconds

    def get(self, key):
        if key in self.cache:
            value, timestamp = self.cache[key]
            if (datetime.now() - timestamp).total_seconds() < self.expiration_seconds:
                return value
            else:
                del self.cache[key]
        return None

    def set(self, key, value):
        self.cache[key] = (value, datetime.now())

# Load Sentiment Data
def load_sentiment_data(filename='sentiment_headlines.yaml'):

    if not os.path.exists(filename):
        raise FileNotFoundError(f"YAML file '{filename}' not found.")
    with open(filename, 'r') as file:
        data = yaml.safe_load(file) or []
    return data


# BBC News Data Collection
def fetch_bbc_headlines(category='world'):
    rss_url = f'http://feeds.bbci.co.uk/news/{category}/rss.xml'
    feed = feedparser.parse(rss_url)
    return feed.entries


def filter_headlines_by_time(entries, start_time, end_time):
    filtered_entries = []
    for entry in entries:
        published_time = datetime(*entry.published_parsed[:6], tzinfo=pytz.UTC).astimezone(pytz.timezone('Europe/London'))
        if start_time <= published_time <= end_time:
            filtered_entries.append({
                'title': entry.title,
                'link': entry.link,
                'published': published_time.isoformat(),
                'category': entry.get('category', 'General')
            })
    return filtered_entries


def save_headlines_to_yaml(headlines, filename='bbc_headlines.yaml'):
    if os.path.exists(filename):
        with open(filename, 'r') as file:
            existing_data = yaml.safe_load(file) or []
    else:
        existing_data = []

    # Combine and deduplicate
    seen_titles = set()
    combined_data = existing_data + headlines
    unique_data = [item for item in combined_data if item['title'] not in seen_titles and not seen_titles.add(item['title'])]

    with open(filename, 'w') as file:
        yaml.dump(unique_data, file, sort_keys=False)


def fetch_and_store_headlines(categories):
    now_uk = datetime.now(pytz.timezone('Europe/London'))
    start_time = now_uk.replace(hour=0, minute=0, second=0, microsecond=0)
    all_headlines = []
    for category in categories:
        entries = fetch_bbc_headlines(category)
        filtered_entries = filter_headlines_by_time(entries, start_time, now_uk)
        all_headlines.extend(filtered_entries)
    save_headlines_to_yaml(all_headlines)


# Sentiment Analysis
def compute_sentiment(headlines):
    sentiment_data = []
    for item in headlines:
        headline = item['title']
        blob = TextBlob(headline)
        polarity = blob.sentiment.polarity
        subjectivity = blob.sentiment.subjectivity
        rag_status = 'Green' if polarity > 0.1 else 'Amber' if -0.1 <= polarity <= 0.1 else 'Red'
        emoji = 'ðŸ˜Š' if rag_status == 'Green' else 'ðŸ˜' if rag_status == 'Amber' else 'ðŸ˜ž'
        sentiment_data.append({
            'Title': headline,
            'Link': item['link'],
            'Published': item['published'],
            'Category': item['category'],
            'Polarity': polarity,
            'Subjectivity': subjectivity,
            'RAG_Status': rag_status,
            'Emoji': emoji
        })
    return pd.DataFrame(sentiment_data)


def save_sentiment_to_yaml(sentiment_df, filename='sentiment_headlines.yaml'):
    data = sentiment_df.to_dict(orient='records')
    with open(filename, 'w') as file:
        yaml.dump(data, file, sort_keys=False)

def load_headlines_from_yaml(filename='bbc_headlines.yaml'):
    with open(filename, 'r') as file:
        data = yaml.safe_load(file) or []
    return data

def compute_sentiment(headlines):
    sentiment_data = []
    for item in headlines:
        headline = item['title']
        blob = TextBlob(headline)
        polarity = blob.sentiment.polarity
        subjectivity = blob.sentiment.subjectivity
        rag_status = 'Green' if polarity > 0.1 else 'Amber' if -0.1 <= polarity <= 0.1 else 'Red'
        emoji = 'ðŸ˜Š' if rag_status == 'Green' else 'ðŸ˜' if rag_status == 'Amber' else 'ðŸ˜ž'
        sentiment_data.append({
            'Title': headline,
            'Link': item['link'],
            'Published': item['published'],
            'Category': item['category'],
            'Polarity': polarity,
            'Subjectivity': subjectivity,
            'RAG_Status': rag_status,
            'Emoji': emoji
        })
    return pd.DataFrame(sentiment_data)

def save_sentiment_to_yaml(sentiment_df, filename='sentiment_headlines.yaml'):
    data = sentiment_df.to_dict(orient='records')
    with open(filename, 'w') as file:
        yaml.dump(data, file, sort_keys=False)

# Define a cache for query results
@functools.lru_cache(maxsize=100)  # Cache up to 100 queries
def cached_query(model, query):
    try:
        response = model.generate_content([query])
        return response.text
    except Exception as e:
        print(f"Error in cached query: {e}")
        return None


# Pre-fetch queries for potential headlines
def prefetch_queries(model, query_templates, headlines):
    print("\n--- Pre-fetching Queries ---")
    for headline in headlines:
        for key, template in query_templates.items():
            query = template.format(headline=headline)
            if not cached_query.cache_info().hits:  # Only pre-fetch if not cached
                cached_query(model, query)
                print(f"Pre-fetched query: {query}")
    print("--- Pre-fetching Complete ---\n")


# Fetch headlines and analyze with caching
def analyze_with_caching(model, query_templates, rss_url='https://feeds.bbci.co.uk/news/world/rss.xml', max_headlines=115):
    headlines = get_realtime_headlines(rss_url, max_headlines)
    if not headlines:
        print("No headlines fetched. Please check the RSS feed URL or try again later.")
        return

    # Pre-fetch queries
    prefetch_queries(model, query_templates, headlines)

    print(f"Fetched {len(headlines)} headlines:")
    for i, headline in enumerate(headlines, 1):
        print(f"{i}. {headline}")

    print("\nAnalyzing headlines...")
    for headline in headlines:
        print(f"\nAnalyzing Headline: {headline}")
        for key, template in query_templates.items():
            query = template.format(headline=headline)
            cached_result = cached_query(model, query)
            if cached_result:
                print(f"\nQuery ({key}): {query}")
                print(f"Response (from cache or API): {cached_result}")


# Example usage
try:
    model = GenerativeModel(model_name="models/gemini-1.5-flash-001")
    analyze_with_caching(model, query_templates)
except Exception as e:
    print(f"Pipeline execution error: {e}")

from collections import defaultdict

def batch_queries(headlines, query_templates):

    batched_queries = defaultdict(list)
    for headline in headlines:
        for key, template in query_templates.items():
            query = {
                "type": key,  # e.g., summarization, categorization
                "query": template.format(headline=headline),
                "headline": headline
            }
            batched_queries[key].append(query)
    return batched_queries

def execute_batched_queries(model, batched_queries, max_retries=3):

    results = defaultdict(list)
    for query_type, queries in batched_queries.items():
        print(f"\nExecuting batch for: {query_type} ({len(queries)} queries)")
        batched_texts = [q["query"] for q in queries]
        retries = 0
        while retries < max_retries:
            try:
                # Call the model API with the batch of queries
                responses = model.generate_content(batched_texts)
                for query, response in zip(queries, responses):
                    results[query_type].append({
                        "headline": query["headline"],
                        "query": query["query"],
                        "response": response.text
                    })
                break  # Exit retry loop on success
            except Exception as e:
                retries += 1
                print(f"Error executing batch '{query_type}': {e} (Retry {retries}/{max_retries})")
                if retries == max_retries:
                    print(f"Max retries reached for batch '{query_type}'")
                    for query in queries:
                        results[query_type].append({
                            "headline": query["headline"],
                            "query": query["query"],
                            "response": f"Error: {e}"
                        })
    return results

def analyze_realtime_headlines_with_batching(model, query_templates, rss_url='https://feeds.bbci.co.uk/news/world/rss.xml', max_headlines=5):

    headlines = get_realtime_headlines(rss_url, max_headlines)
    if not headlines:
        print("No headlines fetched. Please check the RSS feed URL or try again later.")
        return

    print(f"\nFetched {len(headlines)} headlines:")
    for i, headline in enumerate(headlines, 1):
        print(f"{i}. {headline}")

    print("\nBatching and analyzing headlines...")
    batched_queries = batch_queries(headlines, query_templates)
    batched_results = execute_batched_queries(model, batched_queries)

    print("\n--- Batched Query Results ---")
    for query_type, results in batched_results.items():
        print(f"\nResults for {query_type}:")
        for result in results:
            print(f"Headline: {result['headline']}")
            print(f"Query: {result['query']}")
            print(f"Response: {result['response']}")
    return batched_results

def load_sentiment_data(filename='sentiment_headlines.yaml'):

    with open(filename, 'r') as file:
        data = yaml.safe_load(file) or []
    return data

def configure_gemini_api():

    try:
        # Attempt to retrieve GEMINI API Key from Kaggle Secrets
        user_secrets = UserSecretsClient()
        gemini_api_key = user_secrets.get_secret("GEMINI")
    except Exception as e:
        print(f"Kaggle Secrets not accessible or not set: {e}")
        gemini_api_key = None

    # Fallback to environment variable if Kaggle Secrets is not available
    if not gemini_api_key:
        gemini_api_key = os.environ.get('GEMINI')

    if not gemini_api_key:
        raise ValueError("GEMINI API Key not found. Please ensure it's set in Kaggle Secrets or environment variables.")

    # Configure the GEMINI API with the retrieved key
    configure(api_key=gemini_api_key)
    print("GEMINI API configured successfully!")

def summarize_headlines(data):
    """
    Summarizes headlines using the GEMINI API with quota and error handling.
    """
    model = genai.GenerativeModel(model_name="models/gemini-1.5-flash-001")
    cache = SimpleCache()
    summaries = []

    for item in data:
        headline = item['Title']
        query = f"Summarize the following headline briefly: '{headline}'"

        # Check cache
        cached_summary = cache.get(query)
        if cached_summary:
            summaries.append({'Title': headline, 'Summary': cached_summary})
            continue

        retries = 0
        max_retries = 3
        backoff_factor = 2

        while retries < max_retries:
            try:
                # Call the GEMINI API
                response = model.generate_content([query])
                summary = response.text if hasattr(response, "text") else "Error: No summary text available"

                # Cache and store the summary
                cache.set(query, summary)
                summaries.append({'Title': headline, 'Summary': summary})
                break
            except Exception as e:
                if "429" in str(e):  # Handle API quota exhaustion
                    print(f"API quota exhausted. Skipping headline: {headline}")
                    summary = "Error: API quota exhausted"
                    summaries.append({'Title': headline, 'Summary': summary})
                    break
                elif retries < max_retries - 1:
                    retries += 1
                    print(f"Retrying... ({retries}/{max_retries})")
                    time.sleep(backoff_factor ** retries)
                else:
                    print(f"Error summarizing headline '{headline}': {e}")
                    summary = f"Error: Unable to summarize - {e}"
                    summaries.append({'Title': headline, 'Summary': summary})
                    break

    return summaries
def save_summaries_to_yaml(summaries, filename='summarized_headlines.yaml'):

    with open(filename, 'w') as file:
        yaml.dump(summaries, file, sort_keys=False)

if __name__ == "__main__":
    try:
        configure_gemini_api()
        data = load_sentiment_data()
        summaries = summarize_headlines(data)
        save_summaries_to_yaml(summaries)
        print("Headlines summarized and saved to summarized_headlines.yaml")
    except Exception as e:
        print(f"Error: {e}")

def load_sentiment_data(filename='sentiment_headlines.yaml'):

    with open(filename, 'r') as file:
        data = yaml.safe_load(file) or []
    return pd.DataFrame(data)

def create_category_chart(df):

    category_counts = df['Category'].value_counts().reset_index()
    category_counts.columns = ['Category', 'Count']
    fig = px.bar(category_counts, x='Category', y='Count', title='Headlines per Category')
    return fig

def create_rag_status_pie_chart(df):

    rag_counts = df['RAG_Status'].value_counts().reset_index()
    rag_counts.columns = ['RAG_Status', 'Count']
    fig = px.pie(rag_counts, names='RAG_Status', values='Count', title='RAG Status Distribution')
    return fig

def create_time_series_chart(df):

    df['Published'] = pd.to_datetime(df['Published'])
    df.sort_values('Published', inplace=True)
    fig = px.line(df, x='Published', y='Polarity', title='Sentiment Polarity Over Time', markers=True)
    return fig

def run_dashboard(df):

    app = Dash(__name__)

    app.layout = html.Div(children=[
        html.H1(children='BBC News Sentiment Dashboard'),
        dcc.Graph(id='category-chart', figure=create_category_chart(df)),
        dcc.Graph(id='rag-status-pie-chart', figure=create_rag_status_pie_chart(df)),
        dcc.Graph(id='time-series-chart', figure=create_time_series_chart(df)),
        html.Label('Select Category:'),
        dcc.Dropdown(
            options=[{'label': cat, 'value': cat} for cat in df['Category'].unique()],
            value=df['Category'].unique()[0],
            id='category-dropdown'
        ),
        dcc.Graph(id='category-time-series')
    ])

    @app.callback(
        Output('category-time-series', 'figure'),
        Input('category-dropdown', 'value')
    )
    def update_category_time_series(selected_category):
        filtered_df = df[df['Category'] == selected_category]
        return create_time_series_chart(filtered_df)

    app.run_server(debug=True)

# Execution
if __name__ == "__main__":
    try:
        # Configure GEMINI API
        configure_gemini_api()

        # Fetch and store BBC headlines
        categories = ['world', 'business', 'technology', 'health', 'entertainment_and_arts', 'science_and_environment']
        fetch_and_store_headlines(categories)

        # Load headlines and perform sentiment analysis
        headlines = load_sentiment_data('bbc_headlines.yaml')
        sentiment_df = compute_sentiment(headlines)
        save_sentiment_to_yaml(sentiment_df)

        print("Sentiment analysis completed and saved to sentiment_headlines.yaml")
    except Exception as e:
        print(f"Error: {e}")

if __name__ == "__main__":
    df = load_sentiment_data()
    run_dashboard(df)



# -*- coding: utf-8 -*-
"""
GeminiFinancialDecisionSupporter
"""

# Imports
import os
import yaml
import pytz
import pandas as pd
import feedparser
from textblob import TextBlob
from datetime import datetime, timedelta
from dash import Dash, html, dcc, Input, Output
import plotly.express as px
import google.generativeai as genai
from kaggle_secrets import UserSecretsClient
import time

# ---------------------------------
# 1. GEMINI API Configuration
# ---------------------------------
def configure_gemini_api():
    """
    Configures the GEMINI API client.
    """
    try:
        user_secrets = UserSecretsClient()
        gemini_api_key = user_secrets.get_secret("GEMINI")
    except Exception as e:
        print(f"Kaggle Secrets not accessible: {e}")
        gemini_api_key = None

    if not gemini_api_key:
        gemini_api_key = os.environ.get('GEMINI')
    if not gemini_api_key:
        raise ValueError("GEMINI API Key not found. Set it in Kaggle Secrets or environment variables.")

    genai.configure(api_key=gemini_api_key)
    print("GEMINI API configured successfully!")

# ---------------------------------
# 2. BBC Data Collection
# ---------------------------------
def fetch_bbc_headlines(category='world'):
    """
    Fetches BBC news headlines for a given category.
    """
    rss_url = f'http://feeds.bbci.co.uk/news/{category}/rss.xml'
    feed = feedparser.parse(rss_url)
    return feed.entries

def filter_headlines_by_time(entries, start_time, end_time):
    """
    Filters headlines based on their publication time.
    """
    filtered_entries = []
    for entry in entries:
        published_time = datetime(*entry.published_parsed[:6], tzinfo=pytz.UTC).astimezone(pytz.timezone('Europe/London'))
        if start_time <= published_time <= end_time:
            filtered_entries.append({
                'title': entry.title,
                'link': entry.link,
                'published': published_time.isoformat(),
                'category': entry.get('category', 'General')
            })
    return filtered_entries

def save_headlines_to_yaml(headlines, filename='bbc_headlines.yaml'):
    """
    Saves headlines to a YAML file.
    """
    if os.path.exists(filename):
        with open(filename, 'r') as file:
            existing_data = yaml.safe_load(file) or []
    else:
        existing_data = []
    combined_data = existing_data + headlines
    unique_data = {item['title']: item for item in combined_data}.values()  # Deduplicate by title
    with open(filename, 'w') as file:
        yaml.dump(list(unique_data), file, sort_keys=False)

def fetch_and_store_headlines(categories):
    """
    Fetches and stores headlines for multiple categories.
    """
    now_uk = datetime.now(pytz.timezone('Europe/London'))
    start_time = now_uk.replace(hour=0, minute=0, second=0, microsecond=0)
    all_headlines = []
    for category in categories:
        entries = fetch_bbc_headlines(category)
        filtered_entries = filter_headlines_by_time(entries, start_time, now_uk)
        all_headlines.extend(filtered_entries)
    save_headlines_to_yaml(all_headlines)

# ---------------------------------
# 3. Sentiment Analysis
# ---------------------------------
def compute_sentiment(headlines):
    """
    Computes sentiment for each headline.
    """
    sentiment_data = []
    for item in headlines:
        headline = item['title']
        blob = TextBlob(headline)
        polarity = blob.sentiment.polarity
        subjectivity = blob.sentiment.subjectivity
        rag_status = 'Green' if polarity > 0.1 else 'Amber' if -0.1 <= polarity <= 0.1 else 'Red'
        emoji = 'ðŸ˜Š' if rag_status == 'Green' else 'ðŸ˜' if rag_status == 'Amber' else 'ðŸ˜ž'
        sentiment_data.append({
            'Title': headline,
            'Link': item['link'],
            'Published': item['published'],
            'Category': item['category'],
            'Polarity': polarity,
            'Subjectivity': subjectivity,
            'RAG_Status': rag_status,
            'Emoji': emoji
        })
    return pd.DataFrame(sentiment_data)

def save_sentiment_to_yaml(sentiment_df, filename='sentiment_headlines.yaml'):
    """
    Saves sentiment analysis results to a YAML file.
    """
    data = sentiment_df.to_dict(orient='records')
    with open(filename, 'w') as file:
        yaml.dump(data, file, sort_keys=False)

# ---------------------------------
# 4. GEMINI Summarization
# ---------------------------------
class SimpleCache:
    """
    A simple in-memory cache with expiration.
    """
    def __init__(self, expiration_seconds=3600):
        self.cache = {}
        self.expiration_seconds = expiration_seconds

    def get(self, key):
        if key in self.cache:
            value, timestamp = self.cache[key]
            if (datetime.now() - timestamp).total_seconds() < self.expiration_seconds:
                return value
            del self.cache[key]  # Expired
        return None

    def set(self, key, value):
        self.cache[key] = (value, datetime.now())

def summarize_headlines(data):
    """
    Summarizes headlines using the GEMINI API.
    """
    model = genai.GenerativeModel(model_name="models/gemini-1.5-flash-001")
    cache = SimpleCache()
    summaries = []
    for item in data:
        headline = item['Title']
        query = f"Summarize the following headline briefly: '{headline}'"
        summary = cache.get(query)
        if not summary:
            try:
                response = model.generate_content([query])
                summary = response.text
                cache.set(query, summary)
            except Exception as e:
                print(f"Error summarizing headline '{headline}': {e}")
                summary = "Error: Unable to summarize"
        summaries.append({'Title': headline, 'Summary': summary})
    return summaries

def save_summaries_to_yaml(summaries, filename='summarized_headlines.yaml'):
    """
    Saves summarized headlines to a YAML file.
    """
    with open(filename, 'w') as file:
        yaml.dump(summaries, file, sort_keys=False)

# ---------------------------------
# 5. Visualization
# ---------------------------------
def save_charts_as_images(df):
    """
    Saves plots as static images for environments without Dash support.
    """
    category_chart = create_category_chart(df)
    rag_status_chart = create_rag_status_pie_chart(df)

    # Save charts as PNG files
    pio.write_image(category_chart, "category_chart.png")
    pio.write_image(rag_status_chart, "rag_status_chart.png")

    print("Charts saved as static images: 'category_chart.png', 'rag_status_chart.png'")

def create_category_chart(df):
    """
    Creates a bar chart showing the count of headlines per category.
    """
    category_counts = df['Category'].value_counts().reset_index()
    category_counts.columns = ['Category', 'Count']
    return px.bar(category_counts, x='Category', y='Count', title='Headlines per Category')

def create_rag_status_pie_chart(df):
    """
    Creates a pie chart showing the distribution of RAG status.
    """
    rag_counts = df['RAG_Status'].value_counts().reset_index()
    rag_counts.columns = ['RAG_Status', 'Count']
    return px.pie(rag_counts, names='RAG_Status', values='Count', title='RAG Status Distribution')

def run_dashboard(df):
    """
    Runs an interactive dashboard using Dash.
    """
    app = Dash(__name__)
    app.layout = html.Div([
        html.H1('BBC News Sentiment Dashboard'),
        dcc.Graph(figure=create_category_chart(df)),
        dcc.Graph(figure=create_rag_status_pie_chart(df)),
    ])
    app.run_server(debug=True)

# ---------------------------------
# 6. Main Execution
# ---------------------------------
if __name__ == "__main__":
    try:
        # Configure GEMINI API
        configure_gemini_api()

        # Step 1: Fetch and store BBC headlines
        categories = ['world', 'business', 'technology', 'health', 'entertainment_and_arts', 'science_and_environment']
        fetch_and_store_headlines(categories)

        # Step 2: Perform sentiment analysis
        headlines = yaml.safe_load(open('bbc_headlines.yaml', 'r'))
        sentiment_df = compute_sentiment(headlines)
        save_sentiment_to_yaml(sentiment_df)

        # Step 3: Summarize headlines
        summaries = summarize_headlines(sentiment_df.to_dict('records'))
        save_summaries_to_yaml(summaries)

        # Step 4: Run visualization dashboard (choose one approach)
        # Option 1: Interactive Dashboard
        # run_dashboard_inline(sentiment_df)

        # Option 2: Export Charts as Static Images
        save_charts_as_images(sentiment_df)

    except Exception as e:
        print(f"Execution error: {e}")

