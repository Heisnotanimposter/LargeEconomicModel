# -*- coding: utf-8 -*-
"""GeminiFinancialDecisionSupporter

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/geminifinancialdecisionsupporter-fc4303e7-fbc8-4018-9000-0ade3525e8d2.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20241129/auto/storage/goog4_request%26X-Goog-Date%3D20241129T041409Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D0fcfaf51a015e32420271c3db450ca740e7dc062afded672e9df7517483f5642de5a1711858247a0e967374d94f5dddd3c1097af5202f25ba00de899cb2f8ccad319587a373e809d564384097ef46ac45364191e07aa2f9297e44855c1ab7c6399f5d2afa4e57b1ec36d4ae8f4153d7ab07212080601aa973a455090fe446489e6b9caec97203194f94c55bc759f59cc5e52369a6434883b9861d519ee248166c11c86a49b702062a2ee3a1c55c733c17f0730a63bbd9ac051b4708c01563936cb82ad976a9f6d35a4aa124200e88168312978beeff4fa3c11640dce99f847a684ee83d3ca7f16f97000fa7e0f4a504749cc2fe563226db3c92f893f99c6b9ba
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

gemini_long_context_path = kagglehub.competition_download('gemini-long-context')

print('Data source import complete.')

!pip install dash
!pip install feedparser

import feedparser
import yaml
import datetime
from datetime import timedelta
import pytz
import os

import yaml
import pandas as pd
from textblob import TextBlob

import os
import yaml
import time
from google.generativeai import GenerativeModel
from google.generativeai import caching
from google.generativeai import configure

import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import yaml
from dash import Dash, html, dcc, Input, Output

import google.generativeai as genai
from google.generativeai import caching
from kaggle_secrets import UserSecretsClient
import datetime

# Imports
import os
import yaml
import feedparser
import pandas as pd
import pytz
from datetime import datetime
from textblob import TextBlob
from dash import Dash, html, dcc, Input, Output
import plotly.express as px
import google.generativeai as genai
from kaggle_secrets import UserSecretsClient
import time

import functools
import time
import feedparser

# GEMINI API Configuration
def configure_gemini_api():
    # Retrieve GEMINI API Key using Kaggle Secrets
    user_secrets = UserSecretsClient()
    gemini_api_key = user_secrets.get_secret("GEMINI")

    # Validate secret retrieval
    if not gemini_api_key:
        raise ValueError("GEMINI API Key not found in Kaggle Secrets. Please ensure it's set correctly.")

    # Configure GEMINI API
    genai.configure(api_key=gemini_api_key)
    print("GEMINI API configured successfully!")



def fetch_bbc_headlines(category='world'):
    rss_url = f'http://feeds.bbci.co.uk/news/{category}/rss.xml'
    feed = feedparser.parse(rss_url)
    return feed.entries

def filter_headlines_by_time(entries, start_time, end_time):
    filtered_entries = []
    for entry in entries:
        published_time = datetime.datetime(*entry.published_parsed[:6], tzinfo=pytz.UTC)
        published_time = published_time.astimezone(pytz.timezone('Europe/London'))
        if start_time <= published_time <= end_time:
            filtered_entries.append({
                'title': entry.title,
                'link': entry.link,
                'published': published_time.isoformat(),
                'category': entry.get('category', 'General')
            })
    return filtered_entries

def fetch_and_store_headlines(categories):
    now_uk = datetime.datetime.now(pytz.timezone('Europe/London'))
    start_time = now_uk.replace(hour=0, minute=0, second=0, microsecond=0)
    end_time = now_uk
    all_headlines = []
    for category in categories:
        entries = fetch_bbc_headlines(category)
        filtered_entries = filter_headlines_by_time(entries, start_time, end_time)
        all_headlines.extend(filtered_entries)
    save_headlines_to_yaml(all_headlines)

def save_headlines_to_yaml(headlines, filename='bbc_headlines.yaml'):
    if os.path.exists(filename):
        with open(filename, 'r') as file:
            existing_data = yaml.safe_load(file) or []
    else:
        existing_data = []
    combined_data = existing_data + headlines
    seen_titles = set()
    unique_data = []
    for item in combined_data:
        if item['title'] not in seen_titles:
            unique_data.append(item)
            seen_titles.add(item['title'])
    with open(filename, 'w') as file:
        yaml.dump(unique_data, file, sort_keys=False)

# Caching Implementation
class SimpleCache:
    def __init__(self, expiration_seconds=3600):
        self.cache = {}
        self.expiration_seconds = expiration_seconds

    def get(self, key):
        if key in self.cache:
            value, timestamp = self.cache[key]
            if (datetime.now() - timestamp).total_seconds() < self.expiration_seconds:
                return value
            else:
                del self.cache[key]
        return None

    def set(self, key, value):
        self.cache[key] = (value, datetime.now())

# Load Sentiment Data
def load_sentiment_data(filename='sentiment_headlines.yaml'):

    if not os.path.exists(filename):
        raise FileNotFoundError(f"YAML file '{filename}' not found.")
    with open(filename, 'r') as file:
        data = yaml.safe_load(file) or []
    return data


# BBC News Data Collection
def fetch_bbc_headlines(category='world'):
    rss_url = f'http://feeds.bbci.co.uk/news/{category}/rss.xml'
    feed = feedparser.parse(rss_url)
    return feed.entries


def filter_headlines_by_time(entries, start_time, end_time):
    filtered_entries = []
    for entry in entries:
        published_time = datetime(*entry.published_parsed[:6], tzinfo=pytz.UTC).astimezone(pytz.timezone('Europe/London'))
        if start_time <= published_time <= end_time:
            filtered_entries.append({
                'title': entry.title,
                'link': entry.link,
                'published': published_time.isoformat(),
                'category': entry.get('category', 'General')
            })
    return filtered_entries


def save_headlines_to_yaml(headlines, filename='bbc_headlines.yaml'):
    if os.path.exists(filename):
        with open(filename, 'r') as file:
            existing_data = yaml.safe_load(file) or []
    else:
        existing_data = []

    # Combine and deduplicate
    seen_titles = set()
    combined_data = existing_data + headlines
    unique_data = [item for item in combined_data if item['title'] not in seen_titles and not seen_titles.add(item['title'])]

    with open(filename, 'w') as file:
        yaml.dump(unique_data, file, sort_keys=False)


def fetch_and_store_headlines(categories):
    now_uk = datetime.now(pytz.timezone('Europe/London'))
    start_time = now_uk.replace(hour=0, minute=0, second=0, microsecond=0)
    all_headlines = []
    for category in categories:
        entries = fetch_bbc_headlines(category)
        filtered_entries = filter_headlines_by_time(entries, start_time, now_uk)
        all_headlines.extend(filtered_entries)
    save_headlines_to_yaml(all_headlines)


# Sentiment Analysis
def compute_sentiment(headlines):
    sentiment_data = []
    for item in headlines:
        headline = item['title']
        blob = TextBlob(headline)
        polarity = blob.sentiment.polarity
        subjectivity = blob.sentiment.subjectivity
        rag_status = 'Green' if polarity > 0.1 else 'Amber' if -0.1 <= polarity <= 0.1 else 'Red'
        emoji = '😊' if rag_status == 'Green' else '😐' if rag_status == 'Amber' else '😞'
        sentiment_data.append({
            'Title': headline,
            'Link': item['link'],
            'Published': item['published'],
            'Category': item['category'],
            'Polarity': polarity,
            'Subjectivity': subjectivity,
            'RAG_Status': rag_status,
            'Emoji': emoji
        })
    return pd.DataFrame(sentiment_data)


def save_sentiment_to_yaml(sentiment_df, filename='sentiment_headlines.yaml'):
    data = sentiment_df.to_dict(orient='records')
    with open(filename, 'w') as file:
        yaml.dump(data, file, sort_keys=False)

def load_headlines_from_yaml(filename='bbc_headlines.yaml'):
    with open(filename, 'r') as file:
        data = yaml.safe_load(file) or []
    return data

def compute_sentiment(headlines):
    sentiment_data = []
    for item in headlines:
        headline = item['title']
        blob = TextBlob(headline)
        polarity = blob.sentiment.polarity
        subjectivity = blob.sentiment.subjectivity
        rag_status = 'Green' if polarity > 0.1 else 'Amber' if -0.1 <= polarity <= 0.1 else 'Red'
        emoji = '😊' if rag_status == 'Green' else '😐' if rag_status == 'Amber' else '😞'
        sentiment_data.append({
            'Title': headline,
            'Link': item['link'],
            'Published': item['published'],
            'Category': item['category'],
            'Polarity': polarity,
            'Subjectivity': subjectivity,
            'RAG_Status': rag_status,
            'Emoji': emoji
        })
    return pd.DataFrame(sentiment_data)

def save_sentiment_to_yaml(sentiment_df, filename='sentiment_headlines.yaml'):
    data = sentiment_df.to_dict(orient='records')
    with open(filename, 'w') as file:
        yaml.dump(data, file, sort_keys=False)

# Define a cache for query results
@functools.lru_cache(maxsize=100)  # Cache up to 100 queries
def cached_query(model, query):
    try:
        response = model.generate_content([query])
        return response.text
    except Exception as e:
        print(f"Error in cached query: {e}")
        return None


# Pre-fetch queries for potential headlines
def prefetch_queries(model, query_templates, headlines):
    print("\n--- Pre-fetching Queries ---")
    for headline in headlines:
        for key, template in query_templates.items():
            query = template.format(headline=headline)
            if not cached_query.cache_info().hits:  # Only pre-fetch if not cached
                cached_query(model, query)
                print(f"Pre-fetched query: {query}")
    print("--- Pre-fetching Complete ---\n")


# Fetch headlines and analyze with caching
def analyze_with_caching(model, query_templates, rss_url='https://feeds.bbci.co.uk/news/world/rss.xml', max_headlines=5):
    headlines = get_realtime_headlines(rss_url, max_headlines)
    if not headlines:
        print("No headlines fetched. Please check the RSS feed URL or try again later.")
        return

    # Pre-fetch queries
    prefetch_queries(model, query_templates, headlines)

    print(f"Fetched {len(headlines)} headlines:")
    for i, headline in enumerate(headlines, 1):
        print(f"{i}. {headline}")

    print("\nAnalyzing headlines...")
    for headline in headlines:
        print(f"\nAnalyzing Headline: {headline}")
        for key, template in query_templates.items():
            query = template.format(headline=headline)
            cached_result = cached_query(model, query)
            if cached_result:
                print(f"\nQuery ({key}): {query}")
                print(f"Response (from cache or API): {cached_result}")


# Example usage
try:
    model = GenerativeModel(model_name="models/gemini-1.5-flash-001")
    analyze_with_caching(model, query_templates)
except Exception as e:
    print(f"Pipeline execution error: {e}")

from collections import defaultdict

def batch_queries(headlines, query_templates):

    batched_queries = defaultdict(list)
    for headline in headlines:
        for key, template in query_templates.items():
            query = {
                "type": key,  # e.g., summarization, categorization
                "query": template.format(headline=headline),
                "headline": headline
            }
            batched_queries[key].append(query)
    return batched_queries

def execute_batched_queries(model, batched_queries, max_retries=3):

    results = defaultdict(list)
    for query_type, queries in batched_queries.items():
        print(f"\nExecuting batch for: {query_type} ({len(queries)} queries)")
        batched_texts = [q["query"] for q in queries]
        retries = 0
        while retries < max_retries:
            try:
                # Call the model API with the batch of queries
                responses = model.generate_content(batched_texts)
                for query, response in zip(queries, responses):
                    results[query_type].append({
                        "headline": query["headline"],
                        "query": query["query"],
                        "response": response.text
                    })
                break  # Exit retry loop on success
            except Exception as e:
                retries += 1
                print(f"Error executing batch '{query_type}': {e} (Retry {retries}/{max_retries})")
                if retries == max_retries:
                    print(f"Max retries reached for batch '{query_type}'")
                    for query in queries:
                        results[query_type].append({
                            "headline": query["headline"],
                            "query": query["query"],
                            "response": f"Error: {e}"
                        })
    return results

def analyze_realtime_headlines_with_batching(model, query_templates, rss_url='https://feeds.bbci.co.uk/news/world/rss.xml', max_headlines=5):

    headlines = get_realtime_headlines(rss_url, max_headlines)
    if not headlines:
        print("No headlines fetched. Please check the RSS feed URL or try again later.")
        return

    print(f"\nFetched {len(headlines)} headlines:")
    for i, headline in enumerate(headlines, 1):
        print(f"{i}. {headline}")

    print("\nBatching and analyzing headlines...")
    batched_queries = batch_queries(headlines, query_templates)
    batched_results = execute_batched_queries(model, batched_queries)

    print("\n--- Batched Query Results ---")
    for query_type, results in batched_results.items():
        print(f"\nResults for {query_type}:")
        for result in results:
            print(f"Headline: {result['headline']}")
            print(f"Query: {result['query']}")
            print(f"Response: {result['response']}")
    return batched_results

def load_sentiment_data(filename='sentiment_headlines.yaml'):

    with open(filename, 'r') as file:
        data = yaml.safe_load(file) or []
    return data

def configure_gemini_api():

    try:
        # Attempt to retrieve GEMINI API Key from Kaggle Secrets
        user_secrets = UserSecretsClient()
        gemini_api_key = user_secrets.get_secret("GEMINI")
    except Exception as e:
        print(f"Kaggle Secrets not accessible or not set: {e}")
        gemini_api_key = None

    # Fallback to environment variable if Kaggle Secrets is not available
    if not gemini_api_key:
        gemini_api_key = os.environ.get('GEMINI')

    if not gemini_api_key:
        raise ValueError("GEMINI API Key not found. Please ensure it's set in Kaggle Secrets or environment variables.")

    # Configure the GEMINI API with the retrieved key
    configure(api_key=gemini_api_key)
    print("GEMINI API configured successfully!")

def summarize_headlines(data):

    model = GenerativeModel(model_name="models/gemini-1.5-flash-001")
    cache = SimpleCache()  # Initialize the custom cache
    summaries = []
    for item in data:
        headline = item['Title']
        query = f"Summarize the following headline briefly: '{headline}'"

        # Check if the query is already cached
        cached_summary = cache.get(query)
        if cached_summary:
            summary = cached_summary
        else:
            retries = 0
            max_retries = 3
            backoff_factor = 2
            while retries < max_retries:
                try:
                    # Call the API and get the response
                    response = model.generate_content([query])

                    # Access the generated text directly
                    summary = response.text if hasattr(response, "text") else "Error: No summary text available"

                    # Cache the query and its response
                    cache.set(query, summary)
                    break
                except Exception as e:
                    retries += 1
                    print(f"Error: {e} (Retry {retries}/{max_retries})")
                    time.sleep(backoff_factor ** retries)
                    if retries == max_retries:
                        summary = "Error: Unable to summarize"
        summaries.append({
            'Title': headline,
            'Summary': summary
        })
    return summaries
def save_summaries_to_yaml(summaries, filename='summarized_headlines.yaml'):

    with open(filename, 'w') as file:
        yaml.dump(summaries, file, sort_keys=False)

if __name__ == "__main__":
    try:
        configure_gemini_api()
        data = load_sentiment_data()
        summaries = summarize_headlines(data)
        save_summaries_to_yaml(summaries)
        print("Headlines summarized and saved to summarized_headlines.yaml")
    except Exception as e:
        print(f"Error: {e}")

def load_sentiment_data(filename='sentiment_headlines.yaml'):

    with open(filename, 'r') as file:
        data = yaml.safe_load(file) or []
    return pd.DataFrame(data)

def create_category_chart(df):

    category_counts = df['Category'].value_counts().reset_index()
    category_counts.columns = ['Category', 'Count']
    fig = px.bar(category_counts, x='Category', y='Count', title='Headlines per Category')
    return fig

def create_rag_status_pie_chart(df):

    rag_counts = df['RAG_Status'].value_counts().reset_index()
    rag_counts.columns = ['RAG_Status', 'Count']
    fig = px.pie(rag_counts, names='RAG_Status', values='Count', title='RAG Status Distribution')
    return fig

def create_time_series_chart(df):

    df['Published'] = pd.to_datetime(df['Published'])
    df.sort_values('Published', inplace=True)
    fig = px.line(df, x='Published', y='Polarity', title='Sentiment Polarity Over Time', markers=True)
    return fig

def run_dashboard(df):

    app = Dash(__name__)

    app.layout = html.Div(children=[
        html.H1(children='BBC News Sentiment Dashboard'),
        dcc.Graph(id='category-chart', figure=create_category_chart(df)),
        dcc.Graph(id='rag-status-pie-chart', figure=create_rag_status_pie_chart(df)),
        dcc.Graph(id='time-series-chart', figure=create_time_series_chart(df)),
        html.Label('Select Category:'),
        dcc.Dropdown(
            options=[{'label': cat, 'value': cat} for cat in df['Category'].unique()],
            value=df['Category'].unique()[0],
            id='category-dropdown'
        ),
        dcc.Graph(id='category-time-series')
    ])

    @app.callback(
        Output('category-time-series', 'figure'),
        Input('category-dropdown', 'value')
    )
    def update_category_time_series(selected_category):
        filtered_df = df[df['Category'] == selected_category]
        return create_time_series_chart(filtered_df)

    app.run_server(debug=True)

# Execution
if __name__ == "__main__":
    try:
        # Configure GEMINI API
        configure_gemini_api()

        # Fetch and store BBC headlines
        categories = ['world', 'business', 'technology', 'health', 'entertainment_and_arts', 'science_and_environment']
        fetch_and_store_headlines(categories)

        # Load headlines and perform sentiment analysis
        headlines = load_sentiment_data('bbc_headlines.yaml')
        sentiment_df = compute_sentiment(headlines)
        save_sentiment_to_yaml(sentiment_df)

        print("Sentiment analysis completed and saved to sentiment_headlines.yaml")
    except Exception as e:
        print(f"Error: {e}")

if __name__ == "__main__":
    df = load_sentiment_data()
    run_dashboard(df)